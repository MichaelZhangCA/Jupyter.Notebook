{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:\r\n",
      " [[ 6.4000001   2.79999995  5.5999999   2.20000005]\n",
      " [ 5.          2.29999995  3.29999995  1.        ]\n",
      " [ 4.9000001   2.5         4.5         1.70000005]\n",
      " [ 4.9000001   3.0999999   1.5         0.1       ]\n",
      " [ 5.69999981  3.79999995  1.70000005  0.30000001]\n",
      " [ 4.4000001   3.20000005  1.29999995  0.2       ]\n",
      " [ 5.4000001   3.4000001   1.5         0.40000001]\n",
      " [ 6.9000001   3.0999999   5.0999999   2.29999995]\n",
      " [ 6.69999981  3.0999999   4.4000001   1.39999998]\n",
      " [ 5.0999999   3.70000005  1.5         0.40000001]\n",
      " [ 5.19999981  2.70000005  3.9000001   1.39999998]\n",
      " [ 6.9000001   3.0999999   4.9000001   1.5       ]\n",
      " [ 5.80000019  4.          1.20000005  0.2       ]\n",
      " [ 5.4000001   3.9000001   1.70000005  0.40000001]\n",
      " [ 7.69999981  3.79999995  6.69999981  2.20000005]\n",
      " [ 6.30000019  3.29999995  4.69999981  1.60000002]\n",
      " [ 6.80000019  3.20000005  5.9000001   2.29999995]\n",
      " [ 7.5999999   3.          6.5999999   2.0999999 ]\n",
      " [ 6.4000001   3.20000005  5.30000019  2.29999995]\n",
      " [ 5.69999981  4.4000001   1.5         0.40000001]\n",
      " [ 6.69999981  3.29999995  5.69999981  2.0999999 ]\n",
      " [ 6.4000001   2.79999995  5.5999999   2.0999999 ]\n",
      " [ 5.4000001   3.9000001   1.29999995  0.40000001]\n",
      " [ 6.0999999   2.5999999   5.5999999   1.39999998]\n",
      " [ 7.19999981  3.          5.80000019  1.60000002]\n",
      " [ 5.19999981  3.5         1.5         0.2       ]\n",
      " [ 5.80000019  2.5999999   4.          1.20000005]\n",
      " [ 5.9000001   3.          5.0999999   1.79999995]\n",
      " [ 5.4000001   3.          4.5         1.5       ]\n",
      " [ 6.69999981  3.          5.          1.70000005]\n",
      " [ 6.30000019  2.29999995  4.4000001   1.29999995]\n",
      " [ 5.0999999   2.5         3.          1.10000002]\n",
      " [ 6.4000001   3.20000005  4.5         1.5       ]\n",
      " [ 6.80000019  3.          5.5         2.0999999 ]\n",
      " [ 6.19999981  2.79999995  4.80000019  1.79999995]\n",
      " [ 6.9000001   3.20000005  5.69999981  2.29999995]\n",
      " [ 6.5         3.20000005  5.0999999   2.        ]\n",
      " [ 5.80000019  2.79999995  5.0999999   2.4000001 ]\n",
      " [ 5.0999999   3.79999995  1.5         0.30000001]\n",
      " [ 4.80000019  3.          1.39999998  0.30000001]\n",
      " [ 7.9000001   3.79999995  6.4000001   2.        ]\n",
      " [ 5.80000019  2.70000005  5.0999999   1.89999998]\n",
      " [ 6.69999981  3.          5.19999981  2.29999995]\n",
      " [ 5.0999999   3.79999995  1.89999998  0.40000001]\n",
      " [ 4.69999981  3.20000005  1.60000002  0.2       ]\n",
      " [ 6.          2.20000005  5.          1.5       ]\n",
      " [ 4.80000019  3.4000001   1.60000002  0.2       ]\n",
      " [ 7.69999981  2.5999999   6.9000001   2.29999995]\n",
      " [ 4.5999999   3.5999999   1.          0.2       ]\n",
      " [ 7.19999981  3.20000005  6.          1.79999995]\n",
      " [ 5.          3.29999995  1.39999998  0.2       ]\n",
      " [ 6.5999999   3.          4.4000001   1.39999998]\n",
      " [ 6.0999999   2.79999995  4.          1.29999995]\n",
      " [ 5.          3.20000005  1.20000005  0.2       ]\n",
      " [ 7.          3.20000005  4.69999981  1.39999998]\n",
      " [ 6.          3.          4.80000019  1.79999995]\n",
      " [ 7.4000001   2.79999995  6.0999999   1.89999998]\n",
      " [ 5.80000019  2.70000005  5.0999999   1.89999998]\n",
      " [ 6.19999981  3.4000001   5.4000001   2.29999995]\n",
      " [ 5.          2.          3.5         1.        ]\n",
      " [ 5.5999999   2.5         3.9000001   1.10000002]\n",
      " [ 6.69999981  3.0999999   5.5999999   2.4000001 ]\n",
      " [ 6.30000019  2.5         5.          1.89999998]\n",
      " [ 6.4000001   3.0999999   5.5         1.79999995]\n",
      " [ 6.19999981  2.20000005  4.5         1.5       ]\n",
      " [ 7.30000019  2.9000001   6.30000019  1.79999995]\n",
      " [ 4.4000001   3.          1.29999995  0.2       ]\n",
      " [ 7.19999981  3.5999999   6.0999999   2.5       ]\n",
      " [ 6.5         3.          5.5         1.79999995]\n",
      " [ 5.          3.4000001   1.5         0.2       ]\n",
      " [ 4.69999981  3.20000005  1.29999995  0.2       ]\n",
      " [ 6.5999999   2.9000001   4.5999999   1.29999995]\n",
      " [ 5.5         3.5         1.29999995  0.2       ]\n",
      " [ 7.69999981  3.          6.0999999   2.29999995]\n",
      " [ 6.0999999   3.          4.9000001   1.79999995]\n",
      " [ 4.9000001   3.0999999   1.5         0.1       ]\n",
      " [ 5.5         2.4000001   3.79999995  1.10000002]\n",
      " [ 5.69999981  2.9000001   4.19999981  1.29999995]\n",
      " [ 6.          2.9000001   4.5         1.5       ]\n",
      " [ 6.4000001   2.70000005  5.30000019  1.89999998]\n",
      " [ 5.4000001   3.70000005  1.5         0.2       ]\n",
      " [ 6.0999999   2.9000001   4.69999981  1.39999998]\n",
      " [ 6.5         2.79999995  4.5999999   1.5       ]\n",
      " [ 5.5999999   2.70000005  4.19999981  1.29999995]\n",
      " [ 6.30000019  3.4000001   5.5999999   2.4000001 ]\n",
      " [ 4.9000001   3.0999999   1.5         0.1       ]\n",
      " [ 6.80000019  2.79999995  4.80000019  1.39999998]\n",
      " [ 5.69999981  2.79999995  4.5         1.29999995]\n",
      " [ 6.          2.70000005  5.0999999   1.60000002]\n",
      " [ 5.          3.5         1.29999995  0.30000001]\n",
      " [ 6.5         3.          5.19999981  2.        ]\n",
      " [ 6.0999999   2.79999995  4.69999981  1.20000005]\n",
      " [ 5.0999999   3.5         1.39999998  0.30000001]\n",
      " [ 4.5999999   3.0999999   1.5         0.2       ]\n",
      " [ 6.5         3.          5.80000019  2.20000005]\n",
      " [ 4.5999999   3.4000001   1.39999998  0.30000001]\n",
      " [ 4.5999999   3.20000005  1.39999998  0.2       ]\n",
      " [ 7.69999981  2.79999995  6.69999981  2.        ]\n",
      " [ 5.9000001   3.20000005  4.80000019  1.79999995]\n",
      " [ 5.0999999   3.79999995  1.60000002  0.2       ]\n",
      " [ 4.9000001   3.          1.39999998  0.2       ]\n",
      " [ 4.9000001   2.4000001   3.29999995  1.        ]\n",
      " [ 4.5         2.29999995  1.29999995  0.30000001]\n",
      " [ 5.80000019  2.70000005  4.0999999   1.        ]\n",
      " [ 5.          3.4000001   1.60000002  0.40000001]\n",
      " [ 5.19999981  3.4000001   1.39999998  0.2       ]\n",
      " [ 5.30000019  3.70000005  1.5         0.2       ]\n",
      " [ 5.          3.5999999   1.39999998  0.2       ]\n",
      " [ 5.5999999   2.9000001   3.5999999   1.29999995]\n",
      " [ 4.80000019  3.0999999   1.60000002  0.2       ]\n",
      " [ 6.30000019  2.70000005  4.9000001   1.79999995]\n",
      " [ 5.69999981  2.79999995  4.0999999   1.29999995]\n",
      " [ 5.          3.          1.60000002  0.2       ]\n",
      " [ 6.30000019  3.29999995  6.          2.5       ]\n",
      " [ 5.          3.5         1.60000002  0.60000002]\n",
      " [ 5.5         2.5999999   4.4000001   1.20000005]\n",
      " [ 5.69999981  3.          4.19999981  1.20000005]\n",
      " [ 4.4000001   2.9000001   1.39999998  0.2       ]\n",
      " [ 4.80000019  3.          1.39999998  0.1       ]\n",
      " [ 5.5         2.4000001   3.70000005  1.        ]]\n",
      "\r\n",
      "test data\r\n",
      " [[ 5.9000001   3.          4.19999981  1.5       ]\n",
      " [ 6.9000001   3.0999999   5.4000001   2.0999999 ]\n",
      " [ 5.0999999   3.29999995  1.70000005  0.5       ]\n",
      " [ 6.          3.4000001   4.5         1.60000002]\n",
      " [ 5.5         2.5         4.          1.29999995]\n",
      " [ 6.19999981  2.9000001   4.30000019  1.29999995]\n",
      " [ 5.5         4.19999981  1.39999998  0.2       ]\n",
      " [ 6.30000019  2.79999995  5.0999999   1.5       ]\n",
      " [ 5.5999999   3.          4.0999999   1.29999995]\n",
      " [ 6.69999981  2.5         5.80000019  1.79999995]\n",
      " [ 7.0999999   3.          5.9000001   2.0999999 ]\n",
      " [ 4.30000019  3.          1.10000002  0.1       ]\n",
      " [ 5.5999999   2.79999995  4.9000001   2.        ]\n",
      " [ 5.5         2.29999995  4.          1.29999995]\n",
      " [ 6.          2.20000005  4.          1.        ]\n",
      " [ 5.0999999   3.5         1.39999998  0.2       ]\n",
      " [ 5.69999981  2.5999999   3.5         1.        ]\n",
      " [ 4.80000019  3.4000001   1.89999998  0.2       ]\n",
      " [ 5.0999999   3.4000001   1.5         0.2       ]\n",
      " [ 5.69999981  2.5         5.          2.        ]\n",
      " [ 5.4000001   3.4000001   1.70000005  0.2       ]\n",
      " [ 5.5999999   3.          4.5         1.5       ]\n",
      " [ 6.30000019  2.9000001   5.5999999   1.79999995]\n",
      " [ 6.30000019  2.5         4.9000001   1.5       ]\n",
      " [ 5.80000019  2.70000005  3.9000001   1.20000005]\n",
      " [ 6.0999999   3.          4.5999999   1.39999998]\n",
      " [ 5.19999981  4.0999999   1.5         0.1       ]\n",
      " [ 6.69999981  3.0999999   4.69999981  1.5       ]\n",
      " [ 6.69999981  3.29999995  5.69999981  2.5       ]\n",
      " [ 6.4000001   2.9000001   4.30000019  1.29999995]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "# data files\n",
    "IRIS_TRAINING = \"data\\\\iris_training.csv\"\n",
    "IRIS_TEST = \"data\\\\iris_test.csv\"\n",
    "\n",
    "# load dataset\n",
    "training_set = base.load_csv_with_header(filename = IRIS_TRAINING, features_dtype=np.float32, target_dtype=np.int)\n",
    "test_set = base.load_csv_with_header(filename = IRIS_TEST, features_dtype=np.float32, target_dtype=np.int)\n",
    "\n",
    "print(\"train data:\\r\\n\", training_set.data)\n",
    "print(\"\\r\\ntest data\\r\\n\", test_set.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '\\\\model\\\\iris_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000027893C54B70>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# specify that all freatures have real-value daa\n",
    "feature_name = \"flower_features\"\n",
    "feature_columns = [tf.feature_column.numeric_column(feature_name, shape=[4])]\n",
    "classifier = tf.estimator.LinearClassifier(feature_columns=feature_columns, n_classes=3, model_dir=\"model\\\\iris_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'flower_features': <tf.Tensor 'Const:0' shape=(120, 4) dtype=float32>}, <tf.Tensor 'Const_1:0' shape=(120,) dtype=int32>)\n"
     ]
    }
   ],
   "source": [
    "# link the data to the model\n",
    "def input_fn(dataset):\n",
    "    def _fn():\n",
    "        features = {feature_name: tf.constant(dataset.data)}\n",
    "        label = tf.constant(dataset.target)\n",
    "        return features, label\n",
    "    return _fn\n",
    "\n",
    "print(input_fn(training_set)())\n",
    "\n",
    "# dataflow: raw data -> input function -> feature columns -> model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Failed to create a directory: \\/model; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-ff73857f8c1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fit done'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0msave_summaries_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_summary_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_config\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           log_step_count_steps=self._config.log_step_count_steps) as mon_sess:\n\u001b[0m\u001b[0;32m    781\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mMonitoredTrainingSession\u001b[1;34m(master, is_chief, checkpoint_dir, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps)\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[0mall_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m   return MonitoredSession(session_creator=session_creator, hooks=all_hooks,\n\u001b[1;32m--> 368\u001b[1;33m                           stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, session_creator, hooks, stop_grace_period_secs)\u001b[0m\n\u001b[0;32m    671\u001b[0m     super(MonitoredSession, self).__init__(\n\u001b[0;32m    672\u001b[0m         \u001b[0msession_creator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, session_creator, hooks, should_recover, stop_grace_period_secs)\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m       \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m     \u001b[1;31m# Create the session.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m     self._coordinated_creator = self._CoordinatedSessionCreator(\n",
      "\u001b[1;32mC:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\basic_session_run_hooks.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 418\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_summary_writer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSummaryWriterCache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkpoint_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_global_step_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_or_create_global_step_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_global_step_tensor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\summary\\writer\\writer_cache.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(logdir)\u001b[0m\n\u001b[0;32m     59\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mlogdir\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mFileWriterCache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         FileWriterCache._cache[logdir] = FileWriter(\n\u001b[1;32m---> 61\u001b[1;33m             logdir, graph=ops.get_default_graph())\n\u001b[0m\u001b[0;32m     62\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mFileWriterCache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\summary\\writer\\writer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, logdir, graph, max_queue, flush_secs, graph_def, filename_suffix)\u001b[0m\n\u001b[0;32m    334\u001b[0m     \"\"\"\n\u001b[0;32m    335\u001b[0m     event_writer = EventFileWriter(logdir, max_queue, flush_secs,\n\u001b[1;32m--> 336\u001b[1;33m                                    filename_suffix)\n\u001b[0m\u001b[0;32m    337\u001b[0m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_writer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\summary\\writer\\event_file_writer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, logdir, max_queue, flush_secs, filename_suffix)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_logdir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogdir\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIsDirectory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_logdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMakeDirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_logdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_queue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQueue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     self._ev_writer = pywrap_tensorflow.EventsWriter(\n",
      "\u001b[1;32mC:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mrecursive_create_dir\u001b[1;34m(dirname)\u001b[0m\n\u001b[0;32m    366\u001b[0m   \"\"\"\n\u001b[0;32m    367\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m     \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRecursivelyCreateDir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    471\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Failed to create a directory: \\/model; No such file or directory"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "classifier.train(input_fn=input_fn(training_set), steps=1000)\n",
    "print('fit done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
